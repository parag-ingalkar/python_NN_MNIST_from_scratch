{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33247c3-194b-4441-8629-64b0614b5fa6",
   "metadata": {},
   "source": [
    "# Neural Network to train MNIST dataset from scratch with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f71f5-8b9a-4fbf-844e-f7ad5dc07766",
   "metadata": {},
   "source": [
    "#### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8bbff6-8828-463a-b715-702946883f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import idx2numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19008741-92a7-4aaf-aa7c-40f6a4fa30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path_train_images = 'MNIST_data/train-images-idx3-ubyte'\n",
    "rel_path_train_labels = 'MNIST_data/train-labels-idx1-ubyte'\n",
    "rel_path_test_images = 'MNIST_data/t10k-images-idx3-ubyte'\n",
    "rel_path_test_labels = 'MNIST_data/t10k-labels-idx1-ubyte'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c593493b-3cb3-45c0-969c-cdc787a4b509",
   "metadata": {},
   "source": [
    "##### Function to get one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f33170-2d26-4a13-a74b-5ddfb2b93cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr):\n",
    "    arr = arr.flatten()\n",
    "    num_classes = np.max(arr) + 1\n",
    "    one_hot = np.eye(num_classes)[arr]\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da7557-9091-48eb-968d-d2e0213daf17",
   "metadata": {},
   "source": [
    "#### Load Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5e53dba-b198-453d-bf94-9611f52f10ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = idx2numpy.convert_from_file(rel_path_train_images)\n",
    "X_train = X_train.reshape((60000, 28*28))    \n",
    "X_train = X_train/255\n",
    "\n",
    "Y_train = idx2numpy.convert_from_file(rel_path_train_labels)\n",
    "Y_train = one_hot_encode(Y_train)\n",
    "\n",
    "X_test = idx2numpy.convert_from_file(rel_path_test_images)\n",
    "X_test = X_test.reshape((10000, 28*28))    \n",
    "X_test = X_test/255\n",
    "\n",
    "Y_test = idx2numpy.convert_from_file(rel_path_test_labels)\n",
    "Y_test = one_hot_encode(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8617b709-c2ad-4d37-a50c-9d26b01c31af",
   "metadata": {},
   "source": [
    "#### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f9e9f6-111c-40d1-b0a6-02ee0676d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a354ec-2157-480f-807d-108893df3808",
   "metadata": {},
   "source": [
    "#### Defining Activation and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a51d051c-0688-4b1a-8273-08c5828fff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "        output = np.maximum(0, input)\n",
    "        return output\n",
    "\n",
    "def grad_relu(input):\n",
    "    grad = np.where(input > 0, 1, 0)\n",
    "    return grad\n",
    "\n",
    "def softmax(inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        return output\n",
    "\n",
    "def ce_loss(y_pred, y_true):\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred + 1e-15), axis = 1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3e2dd-732e-4ab2-9675-fefebfcf16f4",
   "metadata": {},
   "source": [
    "#### Initialize weights and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39c6bf24-c82d-4f28-8579-19d4ae2547e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0057, Accuracy: 0.9292\n",
      "Epoch 2, Loss: 0.0031, Accuracy: 0.9498\n",
      "Epoch 3, Loss: 0.0023, Accuracy: 0.9612\n",
      "Epoch 4, Loss: 0.0019, Accuracy: 0.9686\n",
      "Epoch 5, Loss: 0.0016, Accuracy: 0.9730\n",
      "Epoch 6, Loss: 0.0014, Accuracy: 0.9764\n",
      "Epoch 7, Loss: 0.0012, Accuracy: 0.9791\n",
      "Epoch 8, Loss: 0.0011, Accuracy: 0.9815\n",
      "Epoch 9, Loss: 0.0010, Accuracy: 0.9832\n",
      "Epoch 10, Loss: 0.0009, Accuracy: 0.9845\n"
     ]
    }
   ],
   "source": [
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "B1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "B2 = np.zeros((1, output_size))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for i in range(0, X_train.shape[0], batch_size):\n",
    "        x = X_train[i:i+batch_size]\n",
    "        y = Y_train[i:i+batch_size]\n",
    "    \n",
    "        z1 = np.dot(x, W1) + B1\n",
    "        a1 = relu(z1)\n",
    "        z2 = np.dot(a1, W2) + B2\n",
    "        a2 = softmax(z2)\n",
    "    \n",
    "        batch_loss = ce_loss(a2, y)\n",
    "        \n",
    "        epoch_loss += batch_loss\n",
    "        num_batches += 1\n",
    "    \n",
    "        dz2 = (a2 - y)\n",
    "        dW2 = (1 / batch_size) * np.dot(a1.T, dz2)\n",
    "        dB2 = (1 / batch_size) * np.sum(dz2, axis=0, keepdims=True)\n",
    "    \n",
    "        da1 = np.dot(dz2, W2.T)\n",
    "        dz1 = da1 * grad_relu(z1)\n",
    "        dW1 = (1 / batch_size) * np.dot(x.T, dz1)\n",
    "        dB1 = (1 / batch_size) * np.sum(dz1, axis=0, keepdims=True)\n",
    "    \n",
    "        W2 -= learning_rate * dW2\n",
    "        B2 -= learning_rate * dB2\n",
    "        W1 -= learning_rate * dW1\n",
    "        B1 -= learning_rate * dB1\n",
    "    \n",
    "    \n",
    "    z1 = np.dot(X_train, W1) + B1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + B2\n",
    "    a2 = softmax(z2)\n",
    "    predictions = np.argmax(a2, axis=1)\n",
    "    targets = np.argmax(Y_train, axis=1)\n",
    "    accuracy = np.mean(predictions == targets)\n",
    "\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_loss/X_train.shape[0]:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864b798-3f06-42cf-8714-65991a43737a",
   "metadata": {},
   "source": [
    "#### Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8256677a-5c7c-4009-87db-730aa4d3a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Test Accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "z1_test = np.dot(X_test, W1) + B1\n",
    "a1_test = relu(z1_test)\n",
    "z2_test = np.dot(a1_test, W2) + B2\n",
    "a2_test = softmax(z2_test)\n",
    "\n",
    "predictions_test = np.argmax(a2_test, axis=1)\n",
    "targets_test = np.argmax(Y_test, axis=1)\n",
    "test_acc = np.mean(predictions_test == targets_test)\n",
    "print(f\"Epoch {epoch+1} Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Python 3.12",
   "language": "python",
   "name": "venvml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
